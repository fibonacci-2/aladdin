W1122 23:00:35.210000 3695077 torch/distributed/run.py:803] 
W1122 23:00:35.210000 3695077 torch/distributed/run.py:803] *****************************************
W1122 23:00:35.210000 3695077 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1122 23:00:35.210000 3695077 torch/distributed/run.py:803] *****************************************
******************training
B: 8. T: 512. world size: 4. 
B: 8. T: 512. world size: 4. 
******************training
B: 8. T: 512. world size: 4. 
******************training
B: 8. T: 512. world size: 4. 
******************training
B: 8. T: 512. world size: 4. 
B: 8. T: 512. world size: 4. 
B: 8. T: 512. world size: 4. 
B: 8. T: 512. world size: 4. 
total desired batch size: 245760
=> calculated gradient accumulation steps: 15
found 404 shards for split train
found 1 shards for split val
num decayed parameter tensors: 50, with 134,479,872 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
validation loss: 11.2148
HellaSwag accuracy: 584/2186=0.2672
ArabicMMLU accuracy: 584/2186=0.2672
step     0 | loss: 11.226204 | lr 8.3916e-07 | norm: 10.8917 | dt: 32449.59ms | tok/sec: 7573.59
step     1 | loss: 11.192043 | lr 1.6783e-06 | norm: 10.4558 | dt: 6110.60ms | tok/sec: 40218.64
step     2 | loss: 11.114236 | lr 2.5175e-06 | norm: 10.6089 | dt: 6116.27ms | tok/sec: 40181.32
step     3 | loss: 11.034980 | lr 3.3566e-06 | norm: 9.1372 | dt: 6119.56ms | tok/sec: 40159.75
step     4 | loss: 10.888283 | lr 4.1958e-06 | norm: 8.2537 | dt: 6137.93ms | tok/sec: 40039.54
step     5 | loss: 10.799378 | lr 5.0350e-06 | norm: 6.4097 | dt: 6144.40ms | tok/sec: 39997.42
step     6 | loss: 10.429005 | lr 5.8741e-06 | norm: 14.2433 | dt: 6166.21ms | tok/sec: 39855.92
step     7 | loss: 10.071745 | lr 6.7133e-06 | norm: 19.6090 | dt: 6181.27ms | tok/sec: 39758.81
